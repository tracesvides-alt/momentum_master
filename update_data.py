import pandas as pd
import pickle
import os
import json
from datetime import datetime
import yfinance as yf
import market_logic # Custom Logic Module

def fetch_metadata_batch(tickers):
    """
    è¤‡æ•°ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    Returns: dict {ticker: {'name': str, 'industry': str, 'summary': str}}
    """
    from deep_translator import GoogleTranslator
    import concurrent.futures
    import threading
    
    metadata = {}
    total = len(tickers)
    
    # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå¢—åˆ†æ›´æ–°ã®ãŸã‚ï¼‰
    metadata_path = "data/metadata_cache.json"
    existing_metadata = {}
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                existing_metadata = json.load(f)
            print(f"  â„¹ï¸ Loaded existing metadata for {len(existing_metadata)} tickers.")
        except:
            pass

    # å‡¦ç†å¯¾è±¡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šï¼‰
    targets = []
    skipped_count = 0
    
    # æ—¥æœ¬èªåˆ¤å®šç”¨é–¢æ•°
    def contains_japanese(text):
        for char in text:
            if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FFF':
                return True
        return False

    for i, ticker in enumerate(tickers, 1):
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç¢ºèªï¼ˆã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Šã€ã‹ã¤summaryãŒæ—¥æœ¬èªã‚’å«ã‚€ï¼‰
        if ticker in existing_metadata:
            cached_data = existing_metadata[ticker]
            summary = cached_data.get('summary', '')
            name = cached_data.get('name', '')
            
            # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
            if summary and name and contains_japanese(summary):
                metadata[ticker] = cached_data
                skipped_count += 1
                continue
        targets.append(ticker)
    
    if skipped_count > 0:
        print(f"  â­ï¸ Skipped {skipped_count}/{total} tickers (Cached).")
    
    if not targets:
        return metadata

    print(f"  âš¡ Fetching metadata for {len(targets)} tickers in parallel...")

    # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªã‚«ã‚¦ãƒ³ã‚¿
    lock = threading.Lock()
    completed_count = 0

    def fetch_single(tkr):
        nonlocal completed_count
        res = {'name': tkr, 'industry': '', 'summary': ''}
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥å€¤ã‚’ç¢ºèª
        if tkr in existing_metadata:
             default_val = existing_metadata[tkr]
        else:
             default_val = {'name': tkr, 'industry': '', 'summary': ''}

        try:
            t = yf.Ticker(tkr)
            info = t.info
            
            summary_en = info.get('longBusinessSummary', '')[:160]
            summary_ja = ""
            if summary_en:
                try:
                    translator = GoogleTranslator(source='auto', target='ja')
                    summary_ja = translator.translate(summary_en)
                except:
                    summary_ja = summary_en
            
            res = {
                'name': info.get('shortName', info.get('longName', tkr)),
                'industry': info.get('industry', info.get('sector', '')),
                'summary': summary_ja
            }
        except Exception:
            res = default_val

        with lock:
            completed_count += 1
            if completed_count % 10 == 0 or completed_count == len(targets):
                print(f"    Processing... {completed_count}/{len(targets)}", end='\r')
        
        return tkr, res

    # ä¸¦åˆ—å®Ÿè¡Œ (max_workers=10ç¨‹åº¦ãŒå®‰å…¨)
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_ticker = {executor.submit(fetch_single, t): t for t in targets}
        for future in concurrent.futures.as_completed(future_to_ticker):
            tkr, data = future.result()
            metadata[tkr] = data
            
    print("") # æ”¹è¡Œ
    return metadata

def main():
    print(f"ğŸš€ Starting Data Update: {datetime.now()}")
    
    # 1. å€™è£œå–å¾—
    print("ğŸ“‹ Fetching Candidates...")
    candidates = market_logic.get_momentum_candidates()
    print(f"ğŸ“‹ Candidates Count: {len(candidates)}")
    
    # 2. ãƒ‡ãƒ¼ã‚¿è¨ˆç®—
    print("ğŸ“Š Calculating Metrics...")
    df_metrics, history_dict = market_logic.calculate_momentum_metrics(candidates)
    
    if df_metrics is not None and not df_metrics.empty:
        # 3. ä¿å­˜ (dataãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ä¿å­˜)
        os.makedirs("data", exist_ok=True)
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        csv_path = "data/momentum_cache.csv"
        df_metrics.to_csv(csv_path, index=False)
        print(f"âœ… Saved {csv_path}")
        
        # ãƒãƒ£ãƒ¼ãƒˆç”¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ (Pickleå½¢å¼ãŒè»½ãã¦é€Ÿã„)
        pkl_path = "data/history_cache.pkl"
        with open(pkl_path, "wb") as f:
            pickle.dump(history_dict, f)
        print(f"âœ… Saved {pkl_path}")
        
        # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ä¿å­˜ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        print("ğŸ“ Fetching Metadata for All Candidates...")
        metadata = fetch_metadata_batch(candidates)
        
        metadata_path = "data/metadata_cache.json"
        with open(metadata_path, "w", encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved {metadata_path} ({len(metadata)} tickers)")
        
        # æ›´æ–°æ™‚åˆ»ã‚’è¨˜éŒ²
        txt_path = "data/last_updated.txt"
        # Use JST (UTC+9) for Japan time
        from datetime import timezone, timedelta
        JST = timezone(timedelta(hours=9))
        with open(txt_path, "w") as f:
            f.write(datetime.now(JST).strftime("%Y-%m-%d %H:%M:%S"))
        print(f"âœ… Saved {txt_path}")
            
    else:
        print("âŒ Data update failed (Empty DataFrame)")
        exit(1) # ã‚¨ãƒ©ãƒ¼çµ‚äº†

if __name__ == "__main__":
    main()
